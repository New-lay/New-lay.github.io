---
layout: post
title: 深度学习主流框架一览
tags: Notes
date: 2023-11-13 15:33 +0800
---

Tensorflow 工程化，语言过于底层，因此有必要学习PyTorch（偏向学术）。

Tensorflow：首先要介绍的就是Google 开源的Tensorflow，这是一款使用C++语言开发的开源数学计算软件，使用数据流图（Data Flow Graph）的形式进行计算。图中的节点代表数学运算，而图中的线条表示多维数据数组（tensor）之间的交互。Tensorflow灵活的架构可以部署在一个或多个CPU、GPU的台式及服务器中，或者使用单一的API应用在移动设备中。最初是由研究人员和Google Brainu团队针对机器学习和深度神经网络进行研究而开发，是目前全世界使用人数最多、社区最为庞大的一个框架，因为是Google公司出品，所以维护和更新也比较频繁，并且有着python和C++的接口，教程也非常完善，同时很多论文复现的第一个版本都是基于Tensorflow写的，所以是深度学习框架默认的老大。
由于其语言太过于底层，目前有很多基于Tensorflow的第三方抽象库将Tensorflow的函数进行封装，使其变得简洁，比较有名的几个是Keras，Tflearn，tfslim，以及TensorLayer。

Caffe：由加州大学伯克利的Phd贾杨清开发，全程是Convolutional Architecture for Fast Feature Embadding，是一个清晰而高效的开源深度学习框架，目前由伯克利视觉学中心进行维护。从其名字就可以看出来其对于卷积网络的支持特别好，同时也是用C++写的，但是没有提供python接口，只提供了C++接口。缺点是不够灵活，同时内存占用率高，只提供了C++的接口，目前Caffe的升级版本Caffe2已经开源，修复了一些问题，同时工程水平得到了进一步提高。

Theano：Theano于2008年诞生于蒙特利尔理工学院，其派生出了大量深度学习python软件包，最著名的包括Blocks和Keras。Theano的核心是一个数学表达式的编译器，它知道如何获取你的结构，并使之成为一个使用numpy、高效本地库的高效代码，如BLAS和本地代码（C++）均可在CPU或GPU上尽可能快的进行。它是为深度学习中处理大型神经网络算法所需的计算而专门设计的，是这类库的首创之一，被认为是深度学习研究和开发的行业标准。由于目前开发Theano的研究人员大都去了Google参与了Tensorflow的开发，所以Tensorflow就像是Theano的孩子。

Torch：Torch是一个有大量机器学习算法支撑的科学计算框架，其诞生已经有十年之久，但是真正起势得益于Facebook开源了大量Torch的深度学习模块和扩展。Torch的特点在于特别灵活，但是另外一个特殊之处是采用了编程语言Lua，在目前深度学习大部分都采用以python为编程语言的大环境下，一个以Lua为编程语言的框架有着更多的优势，这一项小众的语言增加了学习使用Torch这个框架的成本。而PyTorch的前身的就是Torch，其底层和Torch框架一样，但是使用Python重写了很多内容，不仅更加灵活，支持动态图，也提供了python接口。

MXNet：MXNet的主要作者是李沐，最早就是几个人抱着纯粹对技术和开发的热情做起来的兴趣项目，如今成了亚马逊的官方框架，有着非常好的分布式支持，而且性能特别好，内存占用率低，同时其开发的语言接口不仅仅有python和C++，还有R、Matlab，Scala，JavaScript，等等，可以说能够满足使用任何语言的人。但是MXNet缺点也很明显，教程不够完善，使用的人不多导致社区不大，同时每年很少有比赛和论文是基于MXNet实现的，这使得MXNet的推广力度和知名度不高。

PyTorch： 是Torch7团队开发的，从其名字就可以看出来，其于Torch的不同之处在于PyTorch使用了python作为开发语言。所谓的"Python First"，同样说明它是一个以python为优先的深度学习框架，不仅能够实现强大的GPU加速，同时还支持动态神经网路，这是现在很多主流框架比如Tensorflow等都不支持的。
PyTorch既可以看作是加入了GPU支持的numpy，同时也可以看成一个拥有自动求导功能的强大的深度神经网络，除了Facebook之外，它还可以被Twitter、CMU等机构采用。


为什么使用PyTorch
下面给出4个理由：
1.掌握一个框架并不够，现在深度学习并没有谁拥有绝对的垄断地位，就算是Google也没有，所以只学习Tensorflow并不够。同时现在的研究者使用各个框架的都有，如果要去看他们实现的源码，至少也需要了解他们使用的框架，所以多学一个框架，以备不时之需。

2.Tensorflow与Caffe都是命令式的编程语言，而且是静态的，首先必须构建一个神经网络，然后一次又一次使用同样的结构，如果想要改变网络的结构，就必须从头开始。但是对于PyTorch，通过一次反向求导的技术，可以让你零延迟地任意改变神经网络的行为，尽管这项技术不是PyTorch独有，但是目前为止它实现是最快的，能够为你任何疯狂想法的实现获得最高的速度和最佳的灵活性，这也是PyTorch对比Tensorflow最大的优势。动态图和静态图的区别：TensorFlow 使用静态图，这意味着我们先定义计算图，然后不断使用它，而在 PyTorch 中，每次都会重新构建一个新的计算图；对于使用者来说，两种形式的计算图有着非常大的区别，同时静态图和动态图都有他们各自的优点，比如动态图比较方便debug，使用者能够用任何他们喜欢的方式进行debug，同时非常直观，而静态图是通过先定义后运行的方式，之后再次运行的时候就不再需要重新构建计算图，所以速度会比动态图更快。

3.PyTorch的设计思路是线性、直观且易于实现的，当你执行一行代码时，它会忠实的执行，并没有异步的世界观，所有当你的代码出现问题时，可以通过这些信息轻松快捷地找到出错的代码，不会让你在Debug的时候因为错误的指向或者异步和不透明的引擎浪费太多的时间。

4.PyTorch的代码相对于Tensorflow而言，更加简洁直观，同时对于Tensorflow高度工业化很难看懂的底层代码，PyTorch的源代码要友好很多，更容易看懂。深入API，理解PyTorch底层肯定一个令人高兴的事情。一个底层架构可以看懂的框架，你对其理解会更深。
